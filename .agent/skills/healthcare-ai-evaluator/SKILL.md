---
name: healthcare-ai-evaluator
description: Herramientas y gu√≠as para evaluaci√≥n de modelos de IA m√©dica usando m√©tricas est√°ndar (ROUGE, BERTScore, TBFact) y model-as-judge evaluations.
---

# Healthcare AI Evaluator Skill

## Prop√≥sito

Este skill proporciona herramientas y gu√≠as para **evaluar modelos de IA m√©dica** usando el framework **Microsoft Healthcare AI Model Evaluator**. Incluye m√©tricas autom√°ticas, evaluaci√≥n basada en LLM (model-as-judge), y workflows de validaci√≥n cl√≠nica.

**Repositorio fuente**: `c:\Github-Personal\luiscendan-private\healthcare-ai-model-evaluator`

## Cu√°ndo Usar Este Skill

Activa este skill cuando el usuario:
- Necesite evaluar modelos de IA para aplicaciones m√©dicas
- Quiera calcular m√©tricas de benchmarking (ROUGE, BERTScore, TBFact)
- Busque implementar model-as-judge evaluations
- Necesite validar outputs de modelos m√©dicos
- Quiera comparar m√∫ltiples modelos (A/B testing)
- Requiera workflows de revisi√≥n por expertos cl√≠nicos

## üéØ Capacidades Principales

### 1. M√©tricas Autom√°ticas

#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
- **Uso**: Evaluar res√∫menes m√©dicos, generaci√≥n de reportes
- **M√©tricas**: ROUGE-1, ROUGE-2, ROUGE-L
- **Qu√© mide**: Overlap de n-gramas entre texto generado y referencia

#### BERTScore
- **Uso**: Evaluaci√≥n sem√°ntica de textos m√©dicos
- **Qu√© mide**: Similitud sem√°ntica usando embeddings contextuales
- **Ventaja**: Captura par√°frasis y sin√≥nimos mejor que ROUGE

#### TBFact (Factual Consistency)
- **Uso**: Verificar consistencia factual en res√∫menes m√©dicos
- **Qu√© mide**: Si el resumen contiene informaci√≥n factualmente correcta
- **Cr√≠tico para**: Aplicaciones cl√≠nicas donde la precisi√≥n es vital

#### Exact Match
- **Uso**: Tareas de clasificaci√≥n (diagn√≥stico, categorizaci√≥n)
- **Qu√© mide**: Coincidencia exacta entre predicci√≥n y ground truth

### 2. Model-as-Judge

Evaluaci√≥n basada en LLM (Azure OpenAI) para m√©tricas subjetivas:
- **Claridad**: ¬øEl texto es claro y comprensible?
- **Completitud**: ¬øIncluye toda la informaci√≥n relevante?
- **Precisi√≥n cl√≠nica**: ¬øEs m√©dicamente preciso?
- **Relevancia**: ¬øEs relevante para el contexto cl√≠nico?

### 3. Workflows de Validaci√≥n

- **Expert Review**: Validaci√≥n por profesionales m√©dicos
- **Multi-Reviewer**: Combinaci√≥n de evaluadores humanos y AI
- **A/B Testing**: Comparaci√≥n de m√∫ltiples modelos
- **Arena Interface**: UI para comparaci√≥n lado a lado

## üìö Recursos Disponibles

### Documentaci√≥n de Conocimiento

1. **[healthcare-ai-evaluation.md](file:///c:/Github-Personal/luiscendan-private/.agent/knowledge/healthcare-ai-evaluation.md)**
   - Conceptos de evaluaci√≥n m√©dica
   - Patrones de benchmarking
   - Casos de uso

2. **[evaluation-metrics-guide.md](file:///c:/Github-Personal/luiscendan-private/.agent/knowledge/evaluation-metrics-guide.md)**
   - Gu√≠a detallada de cada m√©trica
   - Cu√°ndo usar cada una
   - Interpretaci√≥n de resultados

### Herramientas Ejecutables

**[healthcare_evaluator.py](file:///c:/Github-Personal/luiscendan-private/.agent/tools/healthcare_evaluator.py)**
- Script Python con funciones para calcular m√©tricas
- Ejecutar evaluaciones model-as-judge
- Comparar outputs de modelos

## üõ†Ô∏è C√≥mo Usar las Herramientas

### Calcular M√©tricas ROUGE y BERTScore

```python
from tools.healthcare_evaluator import calculate_text_metrics

# Evaluar un resumen m√©dico
reference = "Patient presents with acute chest pain..."
generated = "The patient has chest pain..."

metrics = calculate_text_metrics(
    reference=reference,
    generated=generated,
    metrics=["rouge", "bertscore"]
)

print(metrics)
# {
#   "rouge1": 0.75,
#   "rouge2": 0.60,
#   "rougeL": 0.70,
#   "bertscore_f1": 0.85
# }
```

### Evaluar Consistencia Factual (TBFact)

```python
from tools.healthcare_evaluator import evaluate_factual_consistency

source_text = "Patient diagnosed with Type 2 diabetes..."
summary = "Patient has diabetes..."

consistency_score = evaluate_factual_consistency(
    source=source_text,
    summary=summary
)

print(f"Factual consistency: {consistency_score}")
```

### Model-as-Judge Evaluation

```python
from tools.healthcare_evaluator import model_as_judge_evaluate

# Evaluar claridad y precisi√≥n cl√≠nica
result = model_as_judge_evaluate(
    text="Patient presents with acute myocardial infarction...",
    criteria=["clarity", "clinical_accuracy", "completeness"],
    azure_openai_endpoint="https://your-endpoint.openai.azure.com",
    api_key="your-api-key"
)

print(result)
# {
#   "clarity": {"score": 4.5, "reasoning": "..."},
#   "clinical_accuracy": {"score": 5.0, "reasoning": "..."},
#   "completeness": {"score": 4.0, "reasoning": "..."}
# }
```

### Comparar M√∫ltiples Modelos

```python
from tools.healthcare_evaluator import compare_models

models_outputs = {
    "gpt-4": "Patient diagnosed with acute MI...",
    "gpt-3.5": "Patient has heart attack...",
    "custom-model": "Acute myocardial infarction diagnosed..."
}

reference = "Patient presents with acute myocardial infarction..."

comparison = compare_models(
    models_outputs=models_outputs,
    reference=reference,
    metrics=["rouge", "bertscore", "tbfact"]
)

print(comparison)
# Tabla comparativa con scores de cada modelo
```

## üìã Casos de Uso Comunes

### 1. Evaluar Modelo de Resumen de Notas Cl√≠nicas

**Objetivo**: Validar que un modelo genera res√∫menes precisos y completos

**M√©tricas recomendadas**:
- ROUGE (overlap l√©xico)
- BERTScore (similitud sem√°ntica)
- TBFact (consistencia factual)
- Model-as-judge (claridad, completitud)

**Workflow**:
1. Calcular m√©tricas autom√°ticas
2. Ejecutar model-as-judge para m√©tricas subjetivas
3. Revisi√≥n por experto cl√≠nico si es cr√≠tico

### 2. Benchmarking de Modelos de Diagn√≥stico

**Objetivo**: Comparar precisi√≥n de m√∫ltiples modelos

**M√©tricas recomendadas**:
- Exact Match (clasificaci√≥n correcta)
- Model-as-judge (razonamiento cl√≠nico)

**Workflow**:
1. Ejecutar todos los modelos en dataset de prueba
2. Calcular exact match para cada uno
3. Usar model-as-judge para evaluar calidad del razonamiento
4. Comparar resultados en Arena UI

### 3. Validaci√≥n de Generaci√≥n de Reportes Radiol√≥gicos

**Objetivo**: Asegurar que reportes generados son precisos

**M√©tricas recomendadas**:
- BERTScore (similitud sem√°ntica con reportes de referencia)
- TBFact (consistencia factual con hallazgos)
- Model-as-judge (precisi√≥n cl√≠nica, completitud)

**Workflow**:
1. Generar reportes con el modelo
2. Calcular BERTScore vs reportes de radi√≥logos
3. Verificar consistencia factual con TBFact
4. Evaluaci√≥n por radi√≥logo experto

## ‚ö†Ô∏è Consideraciones Importantes

### Privacidad de Datos
- **CR√çTICO**: Todos los datos deben estar **desidentificados** (PHI-free)
- Cumplir con HIPAA, GDPR, y regulaciones locales
- No usar datos que puedan identificar pacientes

### Limitaciones de M√©tricas Autom√°ticas
- ROUGE/BERTScore no capturan precisi√≥n cl√≠nica
- TBFact puede tener falsos positivos/negativos
- **Siempre** combinar con revisi√≥n humana para aplicaciones cr√≠ticas

### Model-as-Judge
- No usar el mismo modelo para generar y evaluar
- Validar que el LLM tiene conocimiento m√©dico adecuado
- Revisar razonamientos, no solo scores

### Validaci√≥n Cl√≠nica
- Para aplicaciones en producci√≥n, **requerido** revisi√≥n por expertos
- M√©tricas autom√°ticas son complementarias, no sustitutos
- Documentar proceso de validaci√≥n

## üîó Enlaces al Repositorio

### C√≥digo Fuente de M√©tricas
- **ROUGE/BERTScore**: `healthcare-ai-model-evaluator/functions/medbench/metrics/text_summarization.py`
- **TBFact**: `healthcare-ai-model-evaluator/functions/medbench/evaluators/tbfact/`
- **Exact Match**: `healthcare-ai-model-evaluator/functions/medbench/metrics/text_exact_match.py`

### Documentaci√≥n
- **README principal**: `healthcare-ai-model-evaluator/README.md`
- **Functions README**: `healthcare-ai-model-evaluator/functions/README.md`
- **Deployment**: `healthcare-ai-model-evaluator/DEPLOYMENT.md`

## üí° Flujo de Trabajo Recomendado

1. **Usuario pregunta sobre evaluaci√≥n de modelos m√©dicos**
   ‚Üí Consulta `healthcare-ai-evaluation.md` para conceptos

2. **Usuario necesita calcular m√©tricas**
   ‚Üí Usa `healthcare_evaluator.py` con ejemplos de c√≥digo

3. **Usuario quiere entender una m√©trica espec√≠fica**
   ‚Üí Referencia `evaluation-metrics-guide.md`

4. **Usuario necesita implementar evaluaci√≥n completa**
   ‚Üí Proporciona workflow completo con m√©tricas autom√°ticas + model-as-judge + revisi√≥n experta

## üìä Ejemplo Completo: Pipeline de Evaluaci√≥n

```python
from tools.healthcare_evaluator import (
    calculate_text_metrics,
    evaluate_factual_consistency,
    model_as_judge_evaluate,
    generate_evaluation_report
)

# 1. Datos de entrada
reference_summary = "Patient diagnosed with Type 2 diabetes..."
model_output = "The patient has diabetes mellitus type 2..."
source_note = "Patient presents with elevated glucose levels..."

# 2. M√©tricas autom√°ticas
auto_metrics = calculate_text_metrics(
    reference=reference_summary,
    generated=model_output,
    metrics=["rouge", "bertscore"]
)

# 3. Consistencia factual
factual_score = evaluate_factual_consistency(
    source=source_note,
    summary=model_output
)

# 4. Model-as-judge
judge_eval = model_as_judge_evaluate(
    text=model_output,
    criteria=["clarity", "clinical_accuracy", "completeness"]
)

# 5. Generar reporte
report = generate_evaluation_report(
    auto_metrics=auto_metrics,
    factual_score=factual_score,
    judge_eval=judge_eval,
    output_path="evaluation_report.json"
)

print("Evaluation complete! Report saved.")
```

## üéì Recursos Adicionales

- **Paper TBFact**: Factual consistency evaluation for medical summarization
- **ROUGE Paper**: Lin, 2004
- **BERTScore Paper**: Zhang et al., 2020
- **Azure OpenAI**: https://learn.microsoft.com/azure/ai-services/openai/

Este skill est√° dise√±ado para hacer la evaluaci√≥n de modelos de IA m√©dica **rigurosa, reproducible y cl√≠nicamente v√°lida**.
