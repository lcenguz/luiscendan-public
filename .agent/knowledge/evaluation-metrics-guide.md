# Gu√≠a de M√©tricas de Evaluaci√≥n - Healthcare AI

Gu√≠a pr√°ctica y detallada de cada m√©trica de evaluaci√≥n para modelos de IA m√©dica.

## üìä ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

### Descripci√≥n

ROUGE mide el **overlap de n-gramas** entre el texto generado y uno o m√°s textos de referencia.

### Variantes

#### ROUGE-1 (Unigrams)
- Mide overlap de palabras individuales
- **F√≥rmula**: (palabras en com√∫n) / (total palabras en referencia)
- **Mejor para**: Evaluar cobertura de vocabulario

#### ROUGE-2 (Bigrams)
- Mide overlap de pares de palabras consecutivas
- **Mejor para**: Evaluar fluidez y estructura

#### ROUGE-L (Longest Common Subsequence)
- Mide la subsecuencia com√∫n m√°s larga
- **Mejor para**: Evaluar orden y estructura general

### Interpretaci√≥n de Scores

| Score | Interpretaci√≥n |
|-------|----------------|
| 0.0 - 0.2 | Muy bajo - Poca similitud |
| 0.2 - 0.4 | Bajo - Similitud limitada |
| 0.4 - 0.6 | Moderado - Similitud aceptable |
| 0.6 - 0.8 | Bueno - Alta similitud |
| 0.8 - 1.0 | Excelente - Muy alta similitud |

### Cu√°ndo Usar

‚úÖ **Usar para**:
- Res√∫menes de notas cl√≠nicas
- Generaci√≥n de reportes m√©dicos
- Respuestas a preguntas factuales
- Cuando existe texto de referencia gold-standard

‚ùå **No usar para**:
- Evaluaci√≥n de precisi√≥n cl√≠nica
- Cuando par√°frasis son v√°lidas
- Textos creativos o abiertos
- Evaluaci√≥n sem√°ntica profunda

### Ejemplo Pr√°ctico

```python
from tools.healthcare_evaluator import calculate_rouge

reference = "Patient diagnosed with Type 2 diabetes mellitus and hypertension"
generated = "Patient has diabetes type 2 and high blood pressure"

scores = calculate_rouge(reference, generated)
print(scores)
# {
#   "rouge1": 0.50,  # 4/8 palabras coinciden
#   "rouge2": 0.14,  # 1/7 bigrams coinciden
#   "rougeL": 0.38   # Subsecuencia com√∫n
# }
```

### Limitaciones

- **No captura sin√≥nimos**: "diabetes" ‚â† "diabetes mellitus"
- **No entiende par√°frasis**: "high blood pressure" ‚â† "hypertension"
- **Sensible a orden**: Cambiar orden reduce score
- **No eval√∫a precisi√≥n**: Score alto no garantiza correcci√≥n m√©dica

---

## üß† BERTScore

### Descripci√≥n

BERTScore mide **similitud sem√°ntica** usando embeddings contextuales de BERT.

### C√≥mo Funciona

1. Genera embeddings para cada token usando BERT
2. Calcula similitud coseno entre embeddings
3. Alinea tokens del texto generado con la referencia
4. Computa precision, recall, y F1

### M√©tricas

- **Precision**: ¬øQu√© proporci√≥n del texto generado es relevante?
- **Recall**: ¬øQu√© proporci√≥n de la referencia est√° cubierta?
- **F1**: Media arm√≥nica de precision y recall

### Interpretaci√≥n de Scores

| Score | Interpretaci√≥n |
|-------|----------------|
| < 0.7 | Baja similitud sem√°ntica |
| 0.7 - 0.8 | Similitud moderada |
| 0.8 - 0.9 | Alta similitud |
| > 0.9 | Muy alta similitud |

### Cu√°ndo Usar

‚úÖ **Usar para**:
- Cuando sin√≥nimos y par√°frasis son v√°lidos
- Evaluaci√≥n sem√°ntica de textos m√©dicos
- Comparaci√≥n de descripciones cl√≠nicas
- Cuando ROUGE es demasiado estricto

‚ùå **No usar para**:
- Evaluaci√≥n de precisi√≥n factual
- Cuando velocidad es cr√≠tica (m√°s lento que ROUGE)
- Recursos computacionales limitados

### Ejemplo Pr√°ctico

```python
from tools.healthcare_evaluator import calculate_bertscore

reference = "acute myocardial infarction with ST elevation"
generated = "heart attack with ST segment elevation"

score = calculate_bertscore(reference, generated)
print(score)
# {
#   "precision": 0.87,
#   "recall": 0.85,
#   "f1": 0.86  # Alta similitud sem√°ntica
# }
```

### Ventajas sobre ROUGE

- ‚úÖ Captura sin√≥nimos m√©dicos
- ‚úÖ Entiende par√°frasis
- ‚úÖ Contexto sem√°ntico
- ‚úÖ M√°s robusto a variaciones

### Limitaciones

- M√°s lento que ROUGE
- Requiere GPU para velocidad √≥ptima
- No garantiza precisi√≥n factual
- Puede dar scores altos a textos incorrectos pero sem√°nticamente similares

---

## ‚úÖ TBFact (Factual Consistency)

### Descripci√≥n

TBFact eval√∫a si el **resumen contiene informaci√≥n factualmente correcta** respecto al texto fuente.

### C√≥mo Funciona

1. Extrae hechos/claims del texto fuente
2. Extrae hechos del resumen generado
3. Verifica si cada hecho del resumen est√° soportado por la fuente
4. Calcula score de consistencia

### Interpretaci√≥n de Scores

| Score | Interpretaci√≥n |
|-------|----------------|
| < 0.5 | Baja consistencia - Muchos errores factuales |
| 0.5 - 0.7 | Consistencia moderada - Algunos errores |
| 0.7 - 0.9 | Alta consistencia - Pocos errores |
| > 0.9 | Muy alta consistencia - Casi sin errores |

### Cu√°ndo Usar

‚úÖ **Usar para**:
- Res√∫menes de notas cl√≠nicas
- Generaci√≥n de reportes radiol√≥gicos
- Cualquier tarea donde precisi√≥n factual es CR√çTICA
- Detecci√≥n de alucinaciones

‚ùå **No usar para**:
- Textos creativos
- Cuando no hay texto fuente de referencia
- Evaluaci√≥n de completitud (solo eval√∫a consistencia)

### Ejemplo Pr√°ctico

```python
from tools.healthcare_evaluator import calculate_tbfact

source = """
Patient presents with chest pain radiating to left arm.
ECG shows ST elevation in leads II, III, aVF.
Troponin levels elevated at 2.5 ng/mL.
Diagnosed with inferior wall myocardial infarction.
"""

summary = "Patient diagnosed with heart attack based on ECG and troponin"

score = calculate_tbfact(source, summary)
print(score)
# 0.85 - Alta consistencia factual
```

### Cr√≠tico Para

- **Aplicaciones cl√≠nicas**: Errores factuales pueden ser peligrosos
- **Res√∫menes m√©dicos**: Precisi√≥n es esencial
- **Reportes de diagn√≥stico**: No puede haber alucinaciones

### Limitaciones

- Puede tener falsos positivos (marca como correcto algo incorrecto)
- Puede tener falsos negativos (marca como incorrecto algo correcto)
- No eval√∫a completitud (solo consistencia)
- Requiere texto fuente

---

## üéØ Exact Match

### Descripci√≥n

Exact Match verifica si la **predicci√≥n coincide exactamente** con el ground truth.

### Interpretaci√≥n

- **1**: Coincidencia exacta
- **0**: No coincide

### Cu√°ndo Usar

‚úÖ **Usar para**:
- Clasificaci√≥n de diagn√≥sticos
- Categorizaci√≥n de condiciones
- Tareas de respuesta √∫nica
- Cuando solo hay una respuesta correcta

‚ùå **No usar para**:
- Generaci√≥n de texto libre
- Cuando m√∫ltiples respuestas son v√°lidas
- Evaluaci√≥n de res√∫menes

### Ejemplo Pr√°ctico

```python
from tools.healthcare_evaluator import calculate_exact_match

ground_truth = "Type 2 Diabetes Mellitus"

# Caso 1: Coincidencia exacta
prediction1 = "Type 2 Diabetes Mellitus"
print(calculate_exact_match(ground_truth, prediction1))  # 1

# Caso 2: No coincide (aunque sem√°nticamente similar)
prediction2 = "Diabetes Mellitus Type 2"
print(calculate_exact_match(ground_truth, prediction2))  # 0
```

### Variantes √ötiles

#### Exact Match (Case-Insensitive)
```python
calculate_exact_match(gt, pred, case_sensitive=False)
```

#### Exact Match (Normalized)
```python
# Normaliza espacios, puntuaci√≥n
calculate_exact_match(gt, pred, normalize=True)
```

### Limitaciones

- Muy estricto
- No captura respuestas sem√°nticamente equivalentes
- Sensible a formato y espacios

---

## ü§ñ Model-as-Judge

### Descripci√≥n

Usa un LLM (como GPT-4) para **evaluar outputs seg√∫n criterios espec√≠ficos**.

### Criterios Comunes

1. **Claridad** (1-5): ¬øEs f√°cil de entender?
2. **Precisi√≥n Cl√≠nica** (1-5): ¬øEs m√©dicamente preciso?
3. **Completitud** (1-5): ¬øIncluye informaci√≥n relevante?
4. **Relevancia** (1-5): ¬øEs relevante para el contexto?
5. **Coherencia** (1-5): ¬øEs l√≥gicamente coherente?

### Interpretaci√≥n de Scores

| Score | Interpretaci√≥n |
|-------|----------------|
| 1 | Muy pobre |
| 2 | Pobre |
| 3 | Aceptable |
| 4 | Bueno |
| 5 | Excelente |

### Cu√°ndo Usar

‚úÖ **Usar para**:
- Evaluar aspectos subjetivos
- Cuando necesitas razonamiento
- M√©tricas autom√°ticas no capturan calidad
- Escalabilidad vs revisi√≥n humana

‚ùå **No usar para**:
- Sustituto de validaci√≥n cl√≠nica
- Cuando m√©tricas objetivas son suficientes
- Presupuesto limitado (costos de API)

### Ejemplo Pr√°ctico

```python
from tools.healthcare_evaluator import model_as_judge

text = """
Patient presents with acute chest pain radiating to left arm,
accompanied by diaphoresis and dyspnea. ECG shows ST elevation.
Diagnosis: Acute myocardial infarction.
"""

result = model_as_judge(
    text=text,
    criteria=["clarity", "clinical_accuracy", "completeness"],
    model="gpt-4"
)

print(result)
# {
#   "clarity": {
#     "score": 5,
#     "reasoning": "Text is clear and well-structured..."
#   },
#   "clinical_accuracy": {
#     "score": 5,
#     "reasoning": "Clinically accurate presentation..."
#   },
#   "completeness": {
#     "score": 4,
#     "reasoning": "Includes key symptoms and findings..."
#   }
# }
```

### Best Practices

1. **Prompts claros y espec√≠ficos**
2. **Solicitar razonamiento** (no solo scores)
3. **No usar mismo modelo** para generar y evaluar
4. **Validar con expertos** humanos
5. **Documentar criterios** de evaluaci√≥n

### Limitaciones

- Sesgos del LLM
- Variabilidad en evaluaciones
- Costos de API
- No sustituye validaci√≥n cl√≠nica

---

## üìã Tabla Comparativa de M√©tricas

| M√©trica | Velocidad | Precisi√≥n Factual | Captura Sem√°ntica | Requiere Referencia | Uso Principal |
|---------|-----------|-------------------|-------------------|---------------------|---------------|
| **ROUGE** | ‚ö°‚ö°‚ö° | ‚ùå | ‚ùå | ‚úÖ | Res√∫menes |
| **BERTScore** | ‚ö°‚ö° | ‚ùå | ‚úÖ | ‚úÖ | Similitud sem√°ntica |
| **TBFact** | ‚ö° | ‚úÖ | ‚ö° | ‚úÖ (fuente) | Consistencia factual |
| **Exact Match** | ‚ö°‚ö°‚ö° | ‚úÖ | ‚ùå | ‚úÖ | Clasificaci√≥n |
| **Model-as-Judge** | ‚ö° | ‚ö° | ‚úÖ | ‚ùå | Evaluaci√≥n subjetiva |

---

## üéØ Recomendaciones por Caso de Uso

### Resumen de Notas Cl√≠nicas
1. **ROUGE** - Overlap l√©xico
2. **BERTScore** - Similitud sem√°ntica
3. **TBFact** - Consistencia factual
4. **Model-as-Judge** - Claridad y completitud

### Clasificaci√≥n de Diagn√≥sticos
1. **Exact Match** - Clasificaci√≥n correcta
2. **Accuracy/F1** - M√©tricas de clasificaci√≥n
3. **Model-as-Judge** - Razonamiento cl√≠nico

### Generaci√≥n de Reportes
1. **BERTScore** - Similitud con reportes de referencia
2. **TBFact** - Consistencia factual
3. **Model-as-Judge** - Precisi√≥n cl√≠nica

### Respuesta a Preguntas
1. **ROUGE/BERTScore** - Similitud con respuestas de referencia
2. **Exact Match** - Para respuestas √∫nicas
3. **Model-as-Judge** - Relevancia y precisi√≥n

---

## üí° Tips Pr√°cticos

1. **Combina m√∫ltiples m√©tricas** - Ninguna m√©trica es perfecta
2. **Establece umbrales** - Define qu√© scores son aceptables
3. **Valida con expertos** - M√©tricas autom√°ticas son complementarias
4. **Documenta decisiones** - Explica por qu√© usas cada m√©trica
5. **Itera y mejora** - Ajusta seg√∫n feedback cl√≠nico

## üîó Ver Tambi√©n

- [healthcare-ai-evaluation.md](./healthcare-ai-evaluation.md) - Gu√≠a completa de evaluaci√≥n
- [healthcare-ai-evaluator skill](../skills/healthcare-ai-evaluator/SKILL.md) - Skill para evaluaci√≥n
- [healthcare_evaluator.py](../tools/healthcare_evaluator.py) - Herramientas ejecutables
