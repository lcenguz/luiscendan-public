# Healthcare AI Evaluation - Gu√≠a de Referencia

## üìã Visi√≥n General

La **evaluaci√≥n de modelos de IA m√©dica** requiere m√©tricas rigurosas y validaci√≥n cl√≠nica para asegurar precisi√≥n, seguridad y utilidad en entornos cl√≠nicos. Este documento proporciona una gu√≠a completa sobre evaluaci√≥n de modelos m√©dicos usando el framework **Microsoft Healthcare AI Model Evaluator**.

**Repositorio**: `c:\Github-Personal\luiscendan-private\healthcare-ai-model-evaluator`

## üéØ Tipos de Evaluaci√≥n

### 1. M√©tricas Autom√°ticas
Evaluaci√≥n cuantitativa sin intervenci√≥n humana:
- **ROUGE**: Overlap l√©xico (n-gramas)
- **BERTScore**: Similitud sem√°ntica
- **BLEU/METEOR**: M√©tricas de traducci√≥n adaptadas
- **Exact Match**: Clasificaci√≥n exacta

### 2. Evaluaci√≥n Basada en LLM (Model-as-Judge)
Uso de modelos de lenguaje para evaluar outputs:
- M√©tricas subjetivas (claridad, completitud)
- Razonamiento cl√≠nico
- Precisi√≥n m√©dica
- Relevancia contextual

### 3. Evaluaci√≥n por Expertos
Validaci√≥n por profesionales m√©dicos:
- Revisi√≥n cl√≠nica
- Validaci√≥n de diagn√≥sticos
- Verificaci√≥n de recomendaciones
- Aprobaci√≥n para uso cl√≠nico

### 4. Evaluaci√≥n de Consistencia Factual
Verificaci√≥n de precisi√≥n factual:
- **TBFact**: Factual consistency para res√∫menes
- Verificaci√≥n de hallazgos vs fuente
- Detecci√≥n de alucinaciones

## üìä M√©tricas Principales

### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

**Qu√© mide**: Overlap de n-gramas entre texto generado y referencia

**Variantes**:
- **ROUGE-1**: Unigrams (palabras individuales)
- **ROUGE-2**: Bigrams (pares de palabras)
- **ROUGE-L**: Longest Common Subsequence

**Rango**: 0.0 - 1.0 (mayor es mejor)

**Cu√°ndo usar**:
- Res√∫menes de notas cl√≠nicas
- Generaci√≥n de reportes m√©dicos
- Respuestas a preguntas m√©dicas

**Limitaciones**:
- No captura similitud sem√°ntica
- Sensible a sin√≥nimos y par√°frasis
- No eval√∫a precisi√≥n cl√≠nica

**Ejemplo**:
```
Referencia: "Patient diagnosed with Type 2 diabetes mellitus"
Generado:   "Patient has diabetes type 2"
ROUGE-1: 0.60 (3/5 palabras coinciden)
ROUGE-2: 0.25 (1/4 bigrams coinciden)
```

### BERTScore

**Qu√© mide**: Similitud sem√°ntica usando embeddings contextuales

**M√©tricas**: Precision, Recall, F1

**Rango**: 0.0 - 1.0 (mayor es mejor)

**Cu√°ndo usar**:
- Cuando sin√≥nimos y par√°frasis son v√°lidos
- Evaluaci√≥n sem√°ntica de textos m√©dicos
- Comparaci√≥n de descripciones cl√≠nicas

**Ventajas sobre ROUGE**:
- Captura par√°frasis
- Entiende sin√≥nimos m√©dicos
- Contexto sem√°ntico

**Limitaciones**:
- M√°s lento que ROUGE
- Requiere m√°s recursos computacionales
- No garantiza precisi√≥n factual

**Ejemplo**:
```
Referencia: "acute myocardial infarction"
Generado:   "heart attack"
ROUGE: Bajo (0 palabras coinciden)
BERTScore: Alto (~0.85, sem√°nticamente similar)
```

### TBFact (Factual Consistency)

**Qu√© mide**: Si el resumen contiene informaci√≥n factualmente correcta respecto a la fuente

**Rango**: 0.0 - 1.0 (mayor es mejor)

**Cu√°ndo usar**:
- Res√∫menes de notas cl√≠nicas
- Generaci√≥n de reportes radiol√≥gicos
- Cualquier tarea donde precisi√≥n factual es cr√≠tica

**C√≥mo funciona**:
1. Extrae hechos del texto fuente
2. Verifica si cada hecho est√° presente en el resumen
3. Calcula score de consistencia

**Cr√≠tico para**:
- Aplicaciones cl√≠nicas
- Res√∫menes de historias m√©dicas
- Reportes de diagn√≥stico

**Limitaciones**:
- Puede tener falsos positivos/negativos
- Requiere texto fuente de referencia
- No eval√∫a completitud

### Exact Match

**Qu√© mide**: Coincidencia exacta entre predicci√≥n y ground truth

**Rango**: 0 o 1 (binario)

**Cu√°ndo usar**:
- Clasificaci√≥n de diagn√≥sticos
- Categorizaci√≥n de condiciones
- Tareas de respuesta √∫nica

**Ejemplo**:
```
Ground truth: "Type 2 Diabetes"
Predicci√≥n:    "Type 2 Diabetes" ‚Üí Exact Match = 1
Predicci√≥n:    "Diabetes Type 2" ‚Üí Exact Match = 0
```

## ü§ñ Model-as-Judge

### Concepto

Usar un LLM (como GPT-4) para evaluar outputs de otros modelos seg√∫n criterios espec√≠ficos.

### Criterios Comunes

1. **Claridad**: ¬øEs el texto claro y comprensible?
2. **Completitud**: ¬øIncluye toda la informaci√≥n relevante?
3. **Precisi√≥n Cl√≠nica**: ¬øEs m√©dicamente preciso?
4. **Relevancia**: ¬øEs relevante para el contexto?
5. **Coherencia**: ¬øEs l√≥gicamente coherente?

### Ventajas

- Eval√∫a aspectos subjetivos
- Proporciona razonamiento
- Flexible para diferentes criterios
- Escala mejor que revisi√≥n humana

### Limitaciones

- Puede tener sesgos del LLM
- No sustituye validaci√≥n cl√≠nica
- Requiere prompts bien dise√±ados
- Costos de API

### Best Practices

1. **No usar el mismo modelo para generar y evaluar**
   - ‚ùå Usar GPT-4 para evaluar outputs de GPT-4
   - ‚úÖ Usar GPT-4 para evaluar outputs de GPT-3.5 o custom models

2. **Proporcionar criterios claros**
   ```
   Eval√∫a el siguiente resumen m√©dico seg√∫n:
   - Claridad (1-5): ¬øEs f√°cil de entender?
   - Precisi√≥n (1-5): ¬øEs m√©dicamente preciso?
   - Completitud (1-5): ¬øIncluye informaci√≥n relevante?
   ```

3. **Solicitar razonamiento**
   ```
   Para cada criterio, proporciona:
   - Score (1-5)
   - Razonamiento detallado
   - Ejemplos espec√≠ficos del texto
   ```

4. **Validar con expertos**
   - Comparar evaluaciones del LLM con expertos humanos
   - Ajustar prompts seg√∫n discrepancias

## üè• Casos de Uso por Tipo de Aplicaci√≥n

### Resumen de Notas Cl√≠nicas

**Objetivo**: Generar res√∫menes concisos de historias m√©dicas

**M√©tricas recomendadas**:
- ROUGE (overlap con res√∫menes de referencia)
- BERTScore (similitud sem√°ntica)
- TBFact (consistencia factual)
- Model-as-judge (claridad, completitud)

**Pipeline**:
1. Generar resumen con modelo
2. Calcular ROUGE y BERTScore vs res√∫menes de referencia
3. Verificar consistencia factual con TBFact
4. Evaluar claridad y completitud con model-as-judge
5. Revisi√≥n por m√©dico para casos cr√≠ticos

### Generaci√≥n de Reportes Radiol√≥gicos

**Objetivo**: Generar reportes a partir de im√°genes m√©dicas

**M√©tricas recomendadas**:
- BERTScore (similitud con reportes de radi√≥logos)
- TBFact (consistencia con hallazgos)
- Model-as-judge (precisi√≥n cl√≠nica, terminolog√≠a)

**Pipeline**:
1. Generar reporte desde imagen
2. Comparar con reportes de radi√≥logos (BERTScore)
3. Verificar consistencia factual
4. Evaluaci√≥n por radi√≥logo experto

### Clasificaci√≥n de Diagn√≥sticos

**Objetivo**: Clasificar condiciones m√©dicas

**M√©tricas recomendadas**:
- Exact Match (clasificaci√≥n correcta)
- Accuracy, Precision, Recall, F1
- Model-as-judge (razonamiento cl√≠nico)

**Pipeline**:
1. Clasificar con modelo
2. Calcular exact match y m√©tricas de clasificaci√≥n
3. Evaluar razonamiento con model-as-judge
4. Validaci√≥n por m√©dico para casos ambiguos

### Respuesta a Preguntas M√©dicas

**Objetivo**: Responder preguntas sobre informaci√≥n m√©dica

**M√©tricas recomendadas**:
- ROUGE/BERTScore (similitud con respuestas de referencia)
- Model-as-judge (precisi√≥n, relevancia)
- Exact Match (para preguntas de respuesta √∫nica)

**Pipeline**:
1. Generar respuesta
2. Comparar con respuestas de referencia
3. Evaluar precisi√≥n y relevancia con model-as-judge
4. Verificaci√≥n por experto para informaci√≥n cr√≠tica

## ‚ö†Ô∏è Consideraciones Cr√≠ticas

### Privacidad y Seguridad

- **HIPAA Compliance**: Todos los datos deben estar desidentificados
- **PHI-Free**: No usar informaci√≥n que identifique pacientes
- **GDPR**: Cumplir con regulaciones de privacidad
- **Auditor√≠a**: Documentar uso de datos

### Limitaciones de M√©tricas Autom√°ticas

- **No garantizan precisi√≥n cl√≠nica**: ROUGE alto ‚â† m√©dicamente correcto
- **Pueden ser enga√±adas**: Modelos pueden optimizar m√©tricas sin ser √∫tiles
- **Contexto importa**: Misma m√©trica puede significar cosas diferentes seg√∫n aplicaci√≥n

### Validaci√≥n Cl√≠nica Obligatoria

Para aplicaciones en producci√≥n:
- **Revisi√≥n por expertos** es REQUERIDA
- M√©tricas autom√°ticas son **complementarias**, no sustitutos
- Documentar proceso de validaci√≥n
- Establecer umbrales de confianza

### Transparencia y Explicabilidad

- Documentar qu√© m√©tricas se usan y por qu√©
- Proporcionar razonamiento de evaluaciones
- Ser transparente sobre limitaciones
- Permitir revisi√≥n humana

## üìö Recursos Adicionales

### Papers de Referencia

- **ROUGE**: Lin, C. Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries
- **BERTScore**: Zhang et al. (2020). BERTScore: Evaluating Text Generation with BERT
- **TBFact**: Factual Consistency Evaluation for Medical Summarization

### Herramientas

- **HuggingFace Evaluate**: https://huggingface.co/docs/evaluate/
- **Azure OpenAI**: https://learn.microsoft.com/azure/ai-services/openai/
- **Healthcare AI Evaluator**: Ver [evaluation-metrics-guide.md](./evaluation-metrics-guide.md)

### Gu√≠as Relacionadas

- [evaluation-metrics-guide.md](./evaluation-metrics-guide.md) - Gu√≠a detallada de m√©tricas
- [healthcare-ai-evaluator skill](../skills/healthcare-ai-evaluator/SKILL.md) - Skill para evaluaci√≥n
- [healthcare_evaluator.py](../tools/healthcare_evaluator.py) - Herramientas ejecutables

## üéØ Resumen Ejecutivo

| M√©trica | Qu√© Mide | Cu√°ndo Usar | Limitaciones |
|---------|----------|-------------|--------------|
| **ROUGE** | Overlap l√©xico | Res√∫menes, reportes | No captura sem√°ntica |
| **BERTScore** | Similitud sem√°ntica | Par√°frasis, sin√≥nimos | No garantiza precisi√≥n factual |
| **TBFact** | Consistencia factual | Res√∫menes m√©dicos | Falsos positivos/negativos |
| **Exact Match** | Coincidencia exacta | Clasificaci√≥n | Muy estricto |
| **Model-as-Judge** | Criterios subjetivos | Claridad, precisi√≥n | Sesgos del LLM |

**Regla de oro**: Combina m√©tricas autom√°ticas + model-as-judge + revisi√≥n experta para evaluaci√≥n robusta.
